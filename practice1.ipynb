{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tSep-1 Tokenization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "def tokenize(text, vocab):\n",
    "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text.split(): Splits a sentence into words (e.g., “hello world”: [“hello”, “world”]).\n",
    "vocab: A dictionary that assigns numbers to words (e.g., {“hello”: 0, “world”: 1, “<UNK>”: 2}).\n",
    "vocab.get(word, vocab[“<UNK>”]): Returns a word’s assigned number. If it’s missing, assigns <UNK> (unknown).\n",
    "Think of this as giving each word an ID, so the model can work with numbers instead of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Embedding Layer\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding(vocab_size, embedding_dim): Creates a table where each word ID maps to a vector.\n",
    "embedding_dim: Defines the length of each vector (e.g., 16 numbers per word).\n",
    "Think of embeddings as assigning each word a personality, so words like happy and joyful get similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Positional Encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_dim: Matches the vector size from the embedding layer.\n",
    "max_seq_len: The longest sentence we’ll handle (e.g., 5000 words).\n",
    "Math (sine and cosine): Creates a pattern of numbers that change based on position (e.g., word 1 gets one pattern, word 2 gets another).\n",
    "forward: Adds these position numbers to the word vectors.\n",
    "Think of this as tagging each word with a position stamp so the model understands word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Self-Attention\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attended_values = torch.bmm(attention_weights, values)\n",
    "        return attended_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query, key, value: Three transformations of the input vectors. Think of them as asking “What do I care about?” (query), “What’s available?” (key), and “What do I take?” (value).\n",
    "scores: Measures how much each word relates to every other word.\n",
    "attention_weights: Turns scores into probabilities (e.g., 70% focus on “how”, 30% on “are”).\n",
    "attended_values: Combines the important parts of the sentence.\n",
    "Think of self-attention as a smart highlighter that finds important words to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Transformer Block\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embedding_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(x + attended)\n",
    "        forwarded = self.feed_forward(x)\n",
    "        x = self.norm2(x + forwarded)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention: The self-attention we just built.\n",
    "feed_forward: A small neural network to process each word further.\n",
    "norm1, norm2: Normalizes the numbers so they don’t get too big or small (like keeping everyone on the same scale).\n",
    "x + attended: Adds the original input to the attention output (a trick called “residual connection”).\n",
    "This is like a brain cell, it listens (attention), thinks (feed-forward), and keeps things stable (normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Full Language Model\n",
    "\n",
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SimpleLLM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_dim, hidden_dim) for _ in range(num_layers)])\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1) # Transpose for positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1) # Transpose back\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_layers: How many transformer blocks to stack (more layers = deeper thinking).\n",
    "output: Turns the final vectors back into word predictions (e.g., probabilities for each word in the vocab).\n",
    "This is the final system, it reads the sentence, understands it, and guesses the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.169268846511841\n",
      "Epoch 10, Loss: 1.00962233543396\n",
      "Epoch 20, Loss: 0.3768315613269806\n",
      "Epoch 30, Loss: 0.1939851939678192\n",
      "Epoch 40, Loss: 0.117584228515625\n",
      "Epoch 50, Loss: 0.0784296989440918\n",
      "Epoch 60, Loss: 0.05578667297959328\n",
      "Epoch 70, Loss: 0.04156886041164398\n",
      "Epoch 80, Loss: 0.03206908702850342\n",
      "Epoch 90, Loss: 0.025435248389840126\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training the Model\n",
    "\n",
    "vocab = {\"hello\": 0, \"world\": 1, \"how\": 2, \"are\": 3, \"you\": 4, \"<UNK>\": 5}\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "model = SimpleLLM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "data = [\"hello world how are you\", \"how are you hello world\"]\n",
    "tokenized_data = [tokenize(sentence, vocab) for sentence in data]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for sentence in tokenized_data:\n",
    "        for i in range(1, len(sentence)):\n",
    "            input_seq = torch.tensor(sentence[:i]).unsqueeze(0)\n",
    "            target = torch.tensor(sentence[i]).unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output[:, -1, :], target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_seq: The words so far (e.g., [0, 1] for “hello world”).\n",
    "target: The next word (e.g., 2 for “how”).\n",
    "loss: How far off the prediction was.\n",
    "optimizer.step(): Updates the model to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello world how are, Predicted: you\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Using the Model\n",
    "\n",
    "input_text = \"Hello world how are\"\n",
    "input_tokens = tokenize(input_text, vocab)\n",
    "input_tensor = torch.tensor(input_tokens).unsqueeze(0)\n",
    "output = model(input_tensor)\n",
    "predicted_token = torch.argmax(output[:, -1, :]).item()\n",
    "print(f\"Input: {input_text}, Predicted: {list(vocab.keys())[list(vocab.values()).index(predicted_token)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transliterationByTanu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
